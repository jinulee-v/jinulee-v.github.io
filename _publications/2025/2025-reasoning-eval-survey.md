---
title:          "Evaluating Step-by-step Reasoning Traces: A Survey"
date:           2025-02-18 00:01:00 +0800
selected:       true
# pub:            "NAACL"
# pub_pre:        "Submitted to"
pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"

# abstract: >-
#   Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, the evaluation criteria remain highly unstandardized, leading to fragmented efforts in developing metrics and meta-evaluation benchmarks. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (groundedness, validity, coherence, and utility). We then categorize metrics based on their implementations, survey which metrics are used for assessing each criterion, and explore whether evaluator models can transfer across different criteria. Finally, we identify key directions for future research.
cover:          /assets/images/covers/2025-reasoning-eval-survey.jpg
authors:
  - Jinu Lee
  - Julia Hockenmaier
links:
  Paper: https://arxiv.org/abs/2502.12289
---
